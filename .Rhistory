#benchmark 500K while having to read in the vcf file
hard_filter(vcfR = read.vcfR("~/Desktop/benchmarking.vcfs/benchmark.500K.vcf"),depth = 5,gq = 30),
#set number of reps and units as seconds
times = 3, unit = "s"
)
#read in 500K benchmarking vcf
x<-read.vcfR("~/Desktop/benchmarking.vcfs/benchmark.500K.vcf")
#now benchmark without having to read in the file itself
z<-microbenchmark(
#benchmark 500K while having to read in the vcf file
hard_filter(vcfR = x, depth = 5, gq = 30),
#set number of reps and units as seconds
times = 3, unit = "s"
)
#convert each to a dataframe
y<-summary(y)
z<-summary(z)
#add these results into the full dataframe
sum.df<-rbind(sum.df,y,z)
sum.df$approach<-rep(c("SNPfiltR+vcfR","SNPfiltR"), times = 8)
sum.df$SNPs<-c(10000,10000,20000,20000,50000,50000,100000,100000,
200000,200000,300000,300000,400000,400000,500000,500000)
#read in VCFtools times
vcftools.times<-read.table("~/Desktop/benchmarking.vcfs/cleantimes.txt", sep = "\t")
View(vcftools.times)
#make column 2 numeric
vcftools.times$V2<-gsub("*m", "", vcftools.times$V2)
View(vcftools.times)
#read in VCFtools times
vcftools.times<-read.table("~/Desktop/benchmarking.vcfs/cleantimes.txt", sep = "\t")
View(vcftools.times)
#make column 2 numeric
vcftools.times$V2<-gsub("0m", "", vcftools.times$V2)
View(vcftools.times)
View(vcftools.times)
vcftools.times$V2<-gsub("1m", "", vcftools.times$V2)
vcftools.times$V2<-gsub("s", "", vcftools.times$V2)
vcftools.times$V2<-as.numeric(as.character(vcftools.times$V2))
vcftools.times$V2[22:24]+60
View(vcftools.times)
View(vcftools.times)
#make column 1 informative about run conditions
vcftools.times$V1<-c(rep(10000, times=3),rep(20000, times=3),rep(50000, times=3),
rep(100000, times=3),rep(200000, times=3),rep(300000, times=3),
rep(400000, times=3),rep(500000, times=3))
View(vcftools.times)
View(sum.df)
#
colnames(vcftools.times)<-c("SNPs","run.time")
View(vcftools.times)
View(vcftools.times)
View(sum.df)
#add column tagging all of these times as coming from vcftools
vcftools.times$method<-rep("vcftools", times=nrow(vcftools.times))
View(vcftools.times)
View(sum.df)
SNPfiltR.times<-sum.df[,c(9,10,4)]
View(SNPfiltR.times)
View(vcftools.times)
View(SNPfiltR.times)
summary(vcftools.times)
#
aggregate(vcftools.times$run.time, list(vcftools.times$SNPs), mean)
#
v<-aggregate(vcftools.times$run.time, list(vcftools.times$SNPs), mean)
View(v)
vcftools.times<-read.table("~/Desktop/benchmarking.vcfs/cleantimes.txt", sep = "\t")
#make column 1 informative about run conditions
vcftools.times$V1<-c(rep(10000, times=3),rep(20000, times=3),rep(50000, times=3),
rep(100000, times=3),rep(200000, times=3),rep(300000, times=3),
rep(400000, times=3),rep(500000, times=3))
#make column 2 numeric
vcftools.times$V2<-gsub("0m", "", vcftools.times$V2)
vcftools.times$V2<-gsub("1m", "", vcftools.times$V2)
vcftools.times$V2<-gsub("s", "", vcftools.times$V2)
vcftools.times$V2<-as.numeric(as.character(vcftools.times$V2))
#account for the last 3 which took over a minute
vcftools.times$V2[22:24]<-vcftools.times$V2[22:24]+60
#give informative column names
colnames(vcftools.times)<-c("SNPs","run.time")
#
v<-aggregate(vcftools.times$run.time, list(vcftools.times$SNPs), mean)
View(v)
vcftools.times<-read.table("~/Desktop/benchmarking.vcfs/cleantimes.txt", sep = "\t")
#make column 1 informative about run conditions
vcftools.times$V1<-c(rep(10000, times=3),rep(20000, times=3),rep(50000, times=3),
rep(100000, times=3),rep(200000, times=3),rep(300000, times=3),
rep(400000, times=3),rep(500000, times=3))
#make column 2 numeric
vcftools.times$V2<-gsub("0m", "", vcftools.times$V2)
vcftools.times$V2<-gsub("1m", "", vcftools.times$V2)
vcftools.times$V2<-gsub("s", "", vcftools.times$V2)
vcftools.times$V2<-as.numeric(as.character(vcftools.times$V2))
#account for the last 3 which took over a minute
vcftools.times$V2[22:24]<-vcftools.times$V2[22:24]+60
View(vcftools.times)
#
v<-aggregate(vcftools.times$V2, list(vcftools.times$V1), mean)
View(v)
#add column tagging all of these times as coming from vcftools
v$method<-rep("vcftools", times=nrow(v))
View(v)
#
v<-aggregate(vcftools.times$V2, list(vcftools.times$V1), mean)
#add column tagging all of these times as coming from vcftools
v$approach<-rep("vcftools", times=nrow(v))
View(v)
#give informative column names
colnames(v)[1:2]<-c("SNPs","run.time")
#give informative column names
colnames(v)[1:2]<-c("SNPs","mean")
#reorder to match column order from microbenchmark
v<-v[,c(3,1,2)]
#rbind dataframes
benchmark.df<-rbind(SNPfiltR.times,v)
View(benchmark.df)
class(benchmark.df$approach)
as.factor(benchmark.df$approach)
benchmark.df$approach<-as.factor(benchmark.df$approach)
class(benchmark.df$SNPs)
class(benchmark.df$mean)
library(ggplot2)
View(benchmark.df)
#visualize the comparative runtimes across SNPs and between approaches
ggplot(benchmark.df, aes(x = SNPs, y = mean, color = approach)) +
geom_point(aes(fill=approach),size=3) +
geom_line(aes(group = approach),
arrow = arrow(type = "closed",
length=unit(0.075, "inches")))
#visualize the comparative runtimes across SNPs and between approaches
ggplot(benchmark.df, aes(x = SNPs, y = mean, color = approach)) +
geom_point(aes(fill=approach),size=3) +
geom_line(aes(group = approach))
#visualize the comparative runtimes across SNPs and between approaches
ggplot(benchmark.df, aes(x = SNPs, y = mean, color = approach)) +
geom_point(aes(fill=approach),size=3) +
geom_line(aes(group = approach))+
theme_classic()
#visualize the comparative runtimes across SNPs and between approaches
ggplot(benchmark.df, aes(x = SNPs, y = mean, color = approach)) +
geom_point(aes(fill=approach),size=3) +
geom_line(aes(group = approach))+
theme_classic()+
ylab("mean runtime (seconds)")
#visualize the comparative runtimes across SNPs and between approaches
ggplot(benchmark.df, aes(x = SNPs, y = mean, color = approach)) +
geom_point(aes(fill=approach),size=3) +
geom_line(aes(group = approach))+
scale_x_log10()+
theme_classic()+
ylab("mean runtime (seconds)")
#visualize the comparative runtimes across SNPs and between approaches
ggplot(benchmark.df, aes(x = SNPs, y = mean, color = approach)) +
geom_point(aes(fill=approach),size=3) +
geom_line(aes(group = approach))+
theme_classic()+
ylab("mean runtime (seconds)")
#visualize the comparative runtimes across SNPs and between approaches
ggplot(benchmark.df, aes(x = SNPs, y = mean, color = approach)) +
geom_point(aes(fill=approach),size=3) +
geom_line(aes(group = approach))+
theme_classic()+
ylab("mean runtime (seconds)")+
#try it with a log10 scaled x axis
ggplot(benchmark.df, aes(x = SNPs, y = mean, color = approach)) +
geom_point(aes(fill=approach),size=3) +
geom_line(aes(group = approach))+
scale_x_log10()+
theme_classic()+
ylab("mean runtime (seconds)")+
scale_fill_discrete(name = "gk")
#visualize the comparative runtimes across SNPs and between approaches
ggplot(benchmark.df, aes(x = SNPs, y = mean, color = approach)) +
geom_point(aes(fill=approach),size=3) +
geom_line(aes(group = approach))+
theme_classic()+
ylab("mean runtime (seconds)")+
#try it with a log10 scaled x axis
ggplot(benchmark.df, aes(x = SNPs, y = mean, color = approach)) +
geom_point(aes(fill=approach),size=3) +
geom_line(aes(group = approach))+
scale_x_log10()+
theme_classic()+
ylab("mean runtime (seconds)")
#try it with a log10 scaled x axis
ggplot(benchmark.df, aes(x = SNPs, y = mean, color = approach)) +
geom_point(aes(fill=approach),size=3) +
geom_line(aes(group = approach))+
scale_x_log10()+
theme_classic()+
ylab("mean runtime (seconds)")+
scale_fill_discrete(name = "gk")
vcftools.times<-read.table("~/Desktop/benchmarking.vcfs/cleantimes.txt", sep = "\t")
#make column 1 informative about run conditions
vcftools.times$V1<-c(rep(10000, times=3),rep(20000, times=3),rep(50000, times=3),
rep(100000, times=3),rep(200000, times=3),rep(300000, times=3),
rep(400000, times=3),rep(500000, times=3))
#make column 2 numeric
vcftools.times$V2<-gsub("0m", "", vcftools.times$V2)
vcftools.times$V2<-gsub("1m", "", vcftools.times$V2)
vcftools.times$V2<-gsub("s", "", vcftools.times$V2)
vcftools.times$V2<-as.numeric(as.character(vcftools.times$V2))
#account for the last 3 which took over a minute
vcftools.times$V2[22:24]<-vcftools.times$V2[22:24]+60
#get the mean of the runtimes across all three replicates, for each vcf file
v<-aggregate(vcftools.times$V2, list(vcftools.times$V1), mean)
#add column tagging all of these times as coming from vcftools
v$approach<-rep("VCFtools", times=nrow(v))
#give informative column names
colnames(v)[1:2]<-c("SNPs","mean")
#reorder to match column order from microbenchmark
v<-v[,c(3,1,2)]
#rbind dataframe with the microbenchmark dataframe
benchmark.df<-rbind(SNPfiltR.times,v)
#make approach a factor for plotting
benchmark.df$approach<-as.factor(benchmark.df$approach)
#final product:
benchmark.df
ggplot(benchmark.df, aes(x = SNPs, y = mean, color = approach)) +
geom_point(aes(fill=approach),size=3) +
geom_line(aes(group = approach))+
theme_classic()+
ylab("mean runtime (seconds)")
#visualize the comparative runtimes across SNPs and between approaches
ggplot(benchmark.df, aes(x = SNPs, y = mean, color = approach)) +
geom_point(aes(fill=approach),size=3) +
geom_line(aes(group = approach))+
theme_classic()+
ylab("mean runtime (seconds)")+
theme(legend.position = c(0.2, 0.8))
#try it with a log10 scaled x axis
ggplot(benchmark.df, aes(x = SNPs, y = mean, color = approach)) +
geom_point(aes(fill=approach),size=3) +
geom_line(aes(group = approach))+
scale_x_log10()+
theme_classic()+
ylab("mean runtime (seconds)")+
theme(legend.position = c(0.2, 0.8))
#visualize the comparative runtimes across SNPs and between approaches
ggplot(benchmark.df, aes(x = SNPs, y = mean, color = approach)) +
geom_point(aes(fill=approach),size=3) +
geom_line(aes(group = approach))+
theme_classic()+
ylab("mean runtime (seconds)")+
xlab("SNPs in input vcf")+
theme(legend.position = c(0.2, 0.8))
#visualize the comparative runtimes across SNPs and between approaches
ggplot(benchmark.df, aes(x = SNPs, y = mean, color = approach)) +
geom_point(aes(fill=approach),size=3) +
geom_line(aes(group = approach))+
theme_classic()+
ylab("mean runtime (seconds)")+
theme(legend.position = c(0.2, 0.8))
runtime.plot<-ggplot(benchmark.df, aes(x = SNPs, y = mean, color = approach)) +
geom_point(aes(fill=approach),size=3) +
geom_line(aes(group = approach))+
theme_classic()+
ylab("mean runtime (seconds)")+
theme(legend.position = c(0.2, 0.8))
ggsave(runtime.plot,
filename = "~/Desktop/SNPfiltR.mol.ecol.resour.submission/comp.runtimes.pdf",
height = 4, width = 6, units = "in")
ggsave(runtime.plot,
filename = "~/Desktop/SNPfiltR.mol.ecol.resour.submission/comp.runtimes.pdf",
height = 3, width = 4, units = "in")
build()
library(devtools)
build_site()
write.csv(benchmark.df, file="~/Desktop/benchmarking.vcfs/benchmark.info.csv", quote = F, row.names = F)
build_site()
SNPfiltR::example_vcfR
library(SNPfiltR)
SNPfiltR::vcfR.example
class(SNPfiltR::vcfR.example)
install.packages(‘SNPfiltR’); data(SNPfiltR::example.vcfR)
install.packages('SNPfiltR'); data(SNPfiltR::example.vcfR)
sum(6,9); data(SNPfiltR::vcfR.example)
SNPfiltR::vcfR.example
data(SNPfiltR::vcfR.example)
data(vcfR.example)
build_site()
build_site()
build_site()
library(pkgdown)
build_site()
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(SNPfiltR)
library(vcfR)
#load the example vcfR object
data(vcfR.example)
### check the metadata present in your vcf
vcfR.example
vcfR.example@fix[1:10,1:8]
vcfR.example@gt[1:10,1:2]
#Load the example popmap file. It is a standard two column popmap, where the first column must be named 'id' and contain individual sample identifiers matching the sample identifiers in the vcf file, and the second column must be named 'pop', and contain a population assignment for each sample.
data(popmap)
popmap
#generate exploratory visualizations of depth and genotype quality for all called genotypes
#hard_filter(vcfR=vcfR.example)
#hard filter to minimum depth of 5, and minimum genotype quality of 30
vcfR<-hard_filter(vcfR=vcfR.example, depth = 5, gq = 30)
#execute allele balance filter
vcfR<-filter_allele_balance(vcfR)
#visualize and pick appropriate max depth cutoff
#max_depth(vcfR)
#not running here to save space on visualizations
#filter vcf by the max depth cutoff you chose
vcfR<-max_depth(vcfR, maxdepth = 100)
#check vcfR to see how many SNPs we have left
vcfR
#run function to visualize samples and return informative data.frame object
miss<-missing_by_sample(vcfR=vcfR)
#run function to drop samples above the threshold we want from the vcf
#here I am setting a relatively lax cutoff
vcfR<-missing_by_sample(vcfR=vcfR, cutoff = .9)
#remove invariant sites generated by sample trimming and genotype filtering
vcfR<-min_mac(vcfR, min.mac = 1)
#update popmap by removing samples that have been filtered out
popmap<-popmap[popmap$id %in% colnames(vcfR@gt)[-1],]
#visualize missing data by SNP and the effect of various cutoffs on the missingness of each sample
missing_by_snp(vcfR)
#assess missing data effects on clustering
assess_missing_data_pca(vcfR = vcfR, popmap = popmap, thresholds = c(.8), clustering = FALSE)
install.packages("adegenet")
install.packages("adegenet")
library(adegenet)
library(SNPfiltR)
library(vcfR)
#assess missing data effects on clustering
assess_missing_data_pca(vcfR = vcfR, popmap = popmap, thresholds = c(.8), clustering = FALSE)
R CMD check
library(SNPfiltR)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(SNPfiltR)
library(vcfR)
#load the example vcfR object
data(vcfR.example)
### check the metadata present in your vcf
vcfR.example
vcfR.example@fix[1:10,1:8]
vcfR.example@gt[1:10,1:2]
#Load the example popmap file. It is a standard two column popmap, where the first column must be named 'id' and contain individual sample identifiers matching the sample identifiers in the vcf file, and the second column must be named 'pop', and contain a population assignment for each sample.
data(popmap)
popmap
#generate exploratory visualizations of depth and genotype quality for all called genotypes
#hard_filter(vcfR=vcfR.example)
#hard filter to minimum depth of 5, and minimum genotype quality of 30
vcfR<-hard_filter(vcfR=vcfR.example, depth = 5, gq = 30)
#execute allele balance filter
vcfR<-filter_allele_balance(vcfR)
#visualize and pick appropriate max depth cutoff
#max_depth(vcfR)
#not running here to save space on visualizations
#filter vcf by the max depth cutoff you chose
vcfR<-max_depth(vcfR, maxdepth = 100)
#check vcfR to see how many SNPs we have left
vcfR
#run function to visualize samples and return informative data.frame object
miss<-missing_by_sample(vcfR=vcfR)
#run function to drop samples above the threshold we want from the vcf
#here I am setting a relatively lax cutoff
vcfR<-missing_by_sample(vcfR=vcfR, cutoff = .9)
#remove invariant sites generated by sample trimming and genotype filtering
vcfR<-min_mac(vcfR, min.mac = 1)
#update popmap by removing samples that have been filtered out
popmap<-popmap[popmap$id %in% colnames(vcfR@gt)[-1],]
#visualize missing data by SNP and the effect of various cutoffs on the missingness of each sample
missing_by_snp(vcfR)
#assess missing data effects on clustering
assess_missing_data_pca(vcfR = vcfR, popmap = popmap, thresholds = c(.8), clustering = FALSE)
update.packages()
library(SNPfiltR)
library(vcfR)
vcfR<-hard_filter(vcfR=vcfR.example, depth = 5, gq = 30)
vcfR<-filter_allele_balance(vcfR)
vcfR<-max_depth(vcfR, maxdepth = 100)
miss<-missing_by_sample(vcfR=vcfR)
#run function to drop samples above the threshold we want from the vcf
#here I am setting a relatively lax cutoff
vcfR<-missing_by_sample(vcfR=vcfR, cutoff = .9)
#remove invariant sites generated by sample trimming and genotype filtering
vcfR<-min_mac(vcfR, min.mac = 1)
#update popmap by removing samples that have been filtered out
popmap<-popmap[popmap$id %in% colnames(vcfR@gt)[-1],]
#this PCA cannot tolerate invariant SNP positions, so check for invariant SNP positions
#convert vcfR to matrix and make numeric
gt.matrix<-vcfR::extract.gt(vcfR)
gt.matrix[gt.matrix == "0/0"]<-0
gt.matrix[gt.matrix == "0/1"]<-1
gt.matrix[gt.matrix == "1/1"]<-2
class(gt.matrix) <- "numeric"
#calc sfs
sfs<-rowSums(gt.matrix, na.rm = TRUE)
#calc number of invariant SNPs
g<-sum(sfs < 1)
miss<-colSums(is.na(gt.matrix))/nrow(gt.matrix)
dfs<-list()
thresholds = c(.8)
i=.8
vcfR.filt<-SNPfiltR::missing_by_snp(vcfR = vcfR,
cutoff = i)
gen<-vcfR::vcfR2genlight(vcfR.filt)
pca<-adegenet::glPca(gen,
nf=length(levels(as.factor(popmap$pop)))+2)
pca.scores<-as.data.frame(pca$scores)
#record pam clustering info directly on PCA
m=c()
for (z in 2:(length(levels(as.factor(popmap$pop)))+2)){
m[z]<-mean(cluster::silhouette(cluster::pam(pca.scores, z))[, "sil_width"])
}
rm(list=ls())
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(SNPfiltR)
library(vcfR)
#load the example vcfR object
data(vcfR.example)
### check the metadata present in your vcf
vcfR.example
vcfR.example@fix[1:10,1:8]
vcfR.example@gt[1:10,1:2]
#Load the example popmap file. It is a standard two column popmap, where the first column must be named 'id' and contain individual sample identifiers matching the sample identifiers in the vcf file, and the second column must be named 'pop', and contain a population assignment for each sample.
data(popmap)
popmap
#generate exploratory visualizations of depth and genotype quality for all called genotypes
#hard_filter(vcfR=vcfR.example)
#hard filter to minimum depth of 5, and minimum genotype quality of 30
vcfR<-hard_filter(vcfR=vcfR.example, depth = 5, gq = 30)
#execute allele balance filter
vcfR<-filter_allele_balance(vcfR)
#visualize and pick appropriate max depth cutoff
#max_depth(vcfR)
#not running here to save space on visualizations
#filter vcf by the max depth cutoff you chose
vcfR<-max_depth(vcfR, maxdepth = 100)
#check vcfR to see how many SNPs we have left
vcfR
#run function to visualize samples and return informative data.frame object
miss<-missing_by_sample(vcfR=vcfR)
#run function to drop samples above the threshold we want from the vcf
#here I am setting a relatively lax cutoff
vcfR<-missing_by_sample(vcfR=vcfR, cutoff = .9)
#remove invariant sites generated by sample trimming and genotype filtering
vcfR<-min_mac(vcfR, min.mac = 1)
#update popmap by removing samples that have been filtered out
popmap<-popmap[popmap$id %in% colnames(vcfR@gt)[-1],]
#visualize missing data by SNP and the effect of various cutoffs on the missingness of each sample
missing_by_snp(vcfR)
i=.8
dfs<-list()
vcfR.filt<-SNPfiltR::missing_by_snp(vcfR = vcfR,
cutoff = i)
#convert vcfR into genlight
gen<-vcfR::vcfR2genlight(vcfR.filt)
#execute PCA using this genlight
#retain number of PC axes equivalent to the number of populations being discriminated + 2
pca<-adegenet::glPca(gen,
nf=length(levels(as.factor(popmap$pop)))+2)
update.packages("adegenet")
vcfR.filt<-SNPfiltR::missing_by_snp(vcfR = vcfR,
cutoff = i)
#convert vcfR into genlight
gen<-vcfR::vcfR2genlight(vcfR.filt)
#execute PCA using this genlight
#retain number of PC axes equivalent to the number of populations being discriminated + 2
pca<-adegenet::glPca(gen,
nf=length(levels(as.factor(popmap$pop)))+2)
pca.scores<-as.data.frame(pca$scores)
pca<-adegenet::glPca(gen,
nf=length(levels(as.factor(popmap$pop)))+2)
pca.scores<-as.data.frame(pca$scores)
View(pca)
pca$scores
pca.scores$pop<-popmap$pop[order(popmap$id == colnames(vcfR@gt)[-1])]
pca.scores$missing<-miss
miss<-colSums(is.na(gt.matrix))/nrow(gt.matrix)
#this PCA cannot tolerate invariant SNP positions, so check for invariant SNP positions
#convert vcfR to matrix and make numeric
gt.matrix<-vcfR::extract.gt(vcfR)
gt.matrix[gt.matrix == "0/0"]<-0
gt.matrix[gt.matrix == "0/1"]<-1
gt.matrix[gt.matrix == "1/1"]<-2
class(gt.matrix) <- "numeric"
#calc sfs
sfs<-rowSums(gt.matrix, na.rm = TRUE)
#calc number of invariant SNPs
g<-sum(sfs < 1)
#If there are invariant SNPs, kill the function, and tell user that invariant SNPs aren't allowed
if (g != 0){
stop("invariant SNPs detected in input vcf. Invariant sites must be filtered prior to input")
}
#calculate missingness by individual
miss<-colSums(is.na(gt.matrix))/nrow(gt.matrix)
pca.scores$missing<-miss
var_frac <- pca$eig/sum(pca$eig)
dfs[[i]]<-pca.scores
View(dfs)
devtools::build()
check_for_cran()
rhub::check_for_cran()
R CMD check
build()
devtools::build()
rhub::check_for_cran("/Users/devder/Desktop/SNPfiltR_0.1.1.tar.gz")
